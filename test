https://drive.google.com/file/d/1R7R4SmwC-nEubnIvB0zQnHvHQxLvdRHF/view?usp=drivesdk

from concurrent.futures import ThreadPoolExecutor
from google.cloud import bigquery

client = bigquery.Client()

def call_procedure(start_pos, end_pos):
    query = f"CALL `project.dataset.your_procedure`({start_pos}, {end_pos})"
    query_job = client.query(query)
    query_job.result()

ranges = [(0, 100), (101, 200), (201, 300), (301, 400), (401, 500)]

with ThreadPoolExecutor(max_workers=5) as executor:
    executor.map(lambda r: call_procedure(*r), ranges)


#!/bin/bash

# Define an array of projects, datasets, and tables
declare -a source_projects=("source_project1" "source_project2" "source_project3")
declare -a source_datasets=("source_dataset1" "source_dataset2" "source_dataset3")
declare -a source_tables=("table1" "table2" "table3")
declare -a destination_projects=("dest_project1" "dest_project2" "dest_project3")
declare -a destination_datasets=("dest_dataset1" "dest_dataset2" "dest_dataset3")
declare -a destination_tables=("dest_table1" "dest_table2" "dest_table3")

# Define location (assuming the same location for all operations)
LOCATION="US"

# Loop through the arrays
for i in "${!source_projects[@]}"; do
  echo "Copying ${source_projects[$i]}:${source_datasets[$i]}.${source_tables[$i]} to ${destination_projects[$i]}:${destination_datasets[$i]}.${destination_tables[$i]}"

  # Execute the bq cp command
  bq cp --location=$LOCATION --project_id=${destination_projects[$i]} ${source_projects[$i]}:${source_datasets[$i]}.${source_tables[$i]} ${destination_datasets[$i]}.${destination_tables[$i]}

  # Check if the command was successful
  if [ $? -eq 0 ]; then
    echo "Successfully copied ${source_projects[$i]}:${source_datasets[$i]}.${source_tables[$i]} to ${destination_projects[$i]}:${destination_datasets[$i]}.${destination_tables[$i]}"
  else
    echo "Failed to copy ${source_projects[$i]}:${source_datasets[$i]}.${source_tables[$i]}"
  fi

  echo "-------------------------------------------"
done




#!/bin/bash

# Check if correct number of arguments are passed
if [ "$#" -ne 7 ]; then
  echo "Usage: $0 <source_project> <source_dataset> <source_table> <destination_project> <destination_dataset> <destination_table> <location>"
  exit 1
fi

# Assign arguments to variables
SOURCE_PROJECT=$1
SOURCE_DATASET=$2
SOURCE_TABLE=$3
DEST_PROJECT=$4
DEST_DATASET=$5
DEST_TABLE=$6
LOCATION=$7

# Execute the bq cp command with location
bq cp --location=$LOCATION --project_id=$DEST_PROJECT $SOURCE_PROJECT:$SOURCE_DATASET.$SOURCE_TABLE $DEST_DATASET.$DEST_TABLE

# Check if the command was successful
if [ $? -eq 0 ]; then
  echo "Table copied successfully from $SOURCE_PROJECT:$SOURCE_DATASET.$SOURCE_TABLE to $DEST_PROJECT:$DEST_DATASET.$DEST_TABLE in location $LOCATION"
else
  echo "Failed to copy the table."
fi
#!/bin/bash

# Run the first script without asking for input
echo "Running the first script..."
./script1.sh

# Prompt the user for input before running the next script
read -p "Do you want to continue with the second script? (y/n): " choice

# Check the user's input
if [ "$choice" == "y" ]; then
    echo "Running the second script..."
    ./script2.sh
else
    echo "Cancelling further execution."
    exit 1
fi

# Continue with any additional scripts if needed





#!/bin/bash

# Define variables
PROJECT_ID="your-project-id"
DATASET_ID="your-dataset-id"
TABLE_ID="your-table-id"
PARAMETER_COLUMN="your-column-name"
ROUTINE_NAME="your-routine-name"

# Step 1: Query the parameter value from the table
PARAMETER_VALUE=$(bq --quiet --format=csv query --use_legacy_sql=false \
  "SELECT $PARAMETER_COLUMN FROM \`$PROJECT_ID.$DATASET_ID.$TABLE_ID\` LIMIT 1" | tail -n 1)

# Step 2: Execute the routine using the parameter
bq query --use_legacy_sql=false \
  "CALL \`$PROJECT_ID.$DATASET_ID.$ROUTINE_NAME\`('$PARAMETER_VALUE');"

#!/bin/bash

# Define variables
PROJECT_ID="your-project-id"
DATASET_ID="your-dataset-id"
TABLE_ID="your-table-id"
PARAMETER_COLUMN="your-column-name"
ROUTINE_NAME="your-routine-name"

# Step 1: Query the parameter values from the table
PARAMETER_VALUES=$(bq --quiet --format=csv query --use_legacy_sql=false \
  "SELECT $PARAMETER_COLUMN FROM \`$PROJECT_ID.$DATASET_ID.$TABLE_ID\`")

# Step 2: Iterate over each parameter value and execute the routine
echo "$PARAMETER_VALUES" | while IFS=, read -r PARAMETER_VALUE
do
  if [ "$PARAMETER_VALUE" != "$PARAMETER_COLUMN" ]; then # Skips the header line
    echo "Executing routine with parameter: $PARAMETER_VALUE"
    bq query --use_legacy_sql=false \
      "CALL \`$PROJECT_ID.$DATASET_ID.$ROUTINE_NAME\`('$PARAMETER_VALUE');"
  fi
done


_____<<<<_______<_<<<<<<


#!/bin/bash

# Set BigQuery project and table information
PROJECT_ID="your_project_id"
DATASET_ID="your_dataset_id"
TABLE_ID="email_outbox"

# Temporary file to store email data
EMAIL_DATA_FILE="/tmp/email_data.csv"

# Function to send email using Python
send_email() {
  local email_to="$1"
  local email_subject="$2"
  local email_text="$3"

  python3 <<EOF
import smtplib
from email.mime.text import MIMEText

def send_email(email_to, email_subject, email_text):
    # SMTP server configuration (update these with your server details)
    smtp_server = 'smtp.yourserver.com'
    smtp_port = 587
    smtp_user = 'your_email@domain.com'
    smtp_password = 'your_password'

    msg = MIMEText(email_text)
    msg['Subject'] = email_subject
    msg['From'] = smtp_user
    msg['To'] = email_to

    try:
        with smtplib.SMTP(smtp_server, smtp_port) as server:
            server.starttls()
            server.login(smtp_user, smtp_password)
            server.sendmail(smtp_user, [email_to], msg.as_string())
        print("Email sent successfully")
    except Exception as e:
        print(f"Failed to send email: {e}")

send_email("$email_to", "$email_subject", "$email_text")
EOF
}

# Fetch unsent emails from BigQuery
bq query --use_legacy_sql=false --format=csv "SELECT email_to, email_subject, email_text FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\` WHERE is_sent = 0" > "$EMAIL_DATA_FILE"

# Read the fetched data and process each row
while IFS=, read -r email_to email_subject email_text
do
  if [ "$email_to" != "email_to" ]; then  # Skip the header row
    echo "Sending email to: $email_to"
    send_email "$email_to" "$email_subject" "$email_text"

    # Update the sent flag in BigQuery
    bq query --use_legacy_sql=false "UPDATE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\` SET is_sent = 1 WHERE email_to = '$email_to' AND email_subject = '$email_subject'"
  fi
done < "$EMAIL_DATA_FILE"

# Clean up
rm "$EMAIL_DATA_FILE"

echo "All emails processed."



#!/bin/bash

# Input file with concatenated columns
CONCATENATED_FILE="concatenated_file.csv"
TEMP_CSV_FILE="/tmp/temp_email_data.csv"

# Function to send email using mail command or SMTP client
send_email() {
  local email_to="$1"
  local email_subject="$2"
  local email_text="$3"

  # Use the mail command (adjust to your environment's email-sending method)
  printf "%b" "$email_text" | mail -s "$email_subject" "$email_to"
}

# Convert concatenated file to proper CSV format
# Replace tildes with commas, handle multi-line text
awk -F'~' '
BEGIN { OFS="," }
NR > 1 {
    email_to = $1;
    email_subject = $2;
    email_text = $3;
    
    # Handle potential additional tildes within email_text
    for (i=4; i<=NF; i++) {
        email_text = email_text "~" $i;
    }
    
    # Replace tilde with newline within email_text if needed (example replacement)
    # email_text = gensub(/~/, "\n", "g", email_text);

    # Print in CSV format
    print email_to, email_subject, email_text;
}
' "$CONCATENATED_FILE" > "$TEMP_CSV_FILE"

# Process the CSV file
while IFS=, read -r email_to email_subject email_text
do
  # Remove surrounding quotes from email_text if present
  email_text=$(echo "$email_text" | sed 's/^"//;s/"$//')

  # Convert CSV newlines to actual newlines
  email_text=$(echo "$email_text" | sed 's/\\n/\n/g')

  if [ "$email_to" != "email_to" ]; then  # Skip the header row
    echo "Sending email to: $email_to"
    send_email "$email_to" "$email_subject" "$email_text"

    # Update the sent flag in BigQuery
    bq query --use_legacy_sql=false "UPDATE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\` SET is_sent = 1 WHERE email_to = '$email_to' AND email_subject = '$email_subject'"
  fi
done < "$TEMP_CSV_FILE"

# Clean up
rm "$TEMP_CSV_FILE"

echo "All emails processed."


