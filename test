3https://drive.google.com/file/d/1R7R4SmwC-nEubnIvB0zQnHvHQxLvdRHF/view?usp=drivesdk

from concurrent.futures import ThreadPoolExecutor
from google.cloud import bigquery

client = bigquery.Client()

def call_procedure(start_pos, end_pos):
    query = f"CALL `project.dataset.your_procedure`({start_pos}, {end_pos})"
    query_job = client.query(query)
    query_job.result()

ranges = [(0, 100), (101, 200), (201, 300), (301, 400), (401, 500)]

with ThreadPoolExecutor(max_workers=5) as executor:
    executor.map(lambda r: call_procedure(*r), ranges)


#!/bin/bash

# Define an array of projects, datasets, and tables
declare -a source_projects=("source_project1" "source_project2" "source_project3")
declare -a source_datasets=("source_dataset1" "source_dataset2" "source_dataset3")
declare -a source_tables=("table1" "table2" "table3")
declare -a destination_projects=("dest_project1" "dest_project2" "dest_project3")
declare -a destination_datasets=("dest_dataset1" "dest_dataset2" "dest_dataset3")
declare -a destination_tables=("dest_table1" "dest_table2" "dest_table3")

# Define location (assuming the same location for all operations)
LOCATION="US"

# Loop through the arrays
for i in "${!source_projects[@]}"; do
  echo "Copying ${source_projects[$i]}:${source_datasets[$i]}.${source_tables[$i]} to ${destination_projects[$i]}:${destination_datasets[$i]}.${destination_tables[$i]}"

  # Execute the bq cp command
  bq cp --location=$LOCATION --project_id=${destination_projects[$i]} ${source_projects[$i]}:${source_datasets[$i]}.${source_tables[$i]} ${destination_datasets[$i]}.${destination_tables[$i]}

  # Check if the command was successful
  if [ $? -eq 0 ]; then
    echo "Successfully copied ${source_projects[$i]}:${source_datasets[$i]}.${source_tables[$i]} to ${destination_projects[$i]}:${destination_datasets[$i]}.${destination_tables[$i]}"
  else
    echo "Failed to copy ${source_projects[$i]}:${source_datasets[$i]}.${source_tables[$i]}"
  fi

  echo "-------------------------------------------"
done




#!/bin/bash

# Check if correct number of arguments are passed
if [ "$#" -ne 7 ]; then
  echo "Usage: $0 <source_project> <source_dataset> <source_table> <destination_project> <destination_dataset> <destination_table> <location>"
  exit 1
fi

# Assign arguments to variables
SOURCE_PROJECT=$1
SOURCE_DATASET=$2
SOURCE_TABLE=$3
DEST_PROJECT=$4
DEST_DATASET=$5
DEST_TABLE=$6
LOCATION=$7

# Execute the bq cp command with location
bq cp --location=$LOCATION --project_id=$DEST_PROJECT $SOURCE_PROJECT:$SOURCE_DATASET.$SOURCE_TABLE $DEST_DATASET.$DEST_TABLE

# Check if the command was successful
if [ $? -eq 0 ]; then
  echo "Table copied successfully from $SOURCE_PROJECT:$SOURCE_DATASET.$SOURCE_TABLE to $DEST_PROJECT:$DEST_DATASET.$DEST_TABLE in location $LOCATION"
else
  echo "Failed to copy the table."
fi
#!/bin/bash

# Run the first script without asking for input
echo "Running the first script..."
./script1.sh

# Prompt the user for input before running the next script
read -p "Do you want to continue with the second script? (y/n): " choice

# Check the user's input
if [ "$choice" == "y" ]; then
    echo "Running the second script..."
    ./script2.sh
else
    echo "Cancelling further execution."
    exit 1
fi

# Continue with any additional scripts if needed





#!/bin/bash

# Define variables
PROJECT_ID="your-project-id"
DATASET_ID="your-dataset-id"
TABLE_ID="your-table-id"
PARAMETER_COLUMN="your-column-name"
ROUTINE_NAME="your-routine-name"

# Step 1: Query the parameter value from the table
PARAMETER_VALUE=$(bq --quiet --format=csv query --use_legacy_sql=false \
  "SELECT $PARAMETER_COLUMN FROM \`$PROJECT_ID.$DATASET_ID.$TABLE_ID\` LIMIT 1" | tail -n 1)

# Step 2: Execute the routine using the parameter
bq query --use_legacy_sql=false \
  "CALL \`$PROJECT_ID.$DATASET_ID.$ROUTINE_NAME\`('$PARAMETER_VALUE');"

#!/bin/bash

# Define variables
PROJECT_ID="your-project-id"
DATASET_ID="your-dataset-id"
TABLE_ID="your-table-id"
PARAMETER_COLUMN="your-column-name"
ROUTINE_NAME="your-routine-name"

# Step 1: Query the parameter values from the table
PARAMETER_VALUES=$(bq --quiet --format=csv query --use_legacy_sql=false \
  "SELECT $PARAMETER_COLUMN FROM \`$PROJECT_ID.$DATASET_ID.$TABLE_ID\`")

# Step 2: Iterate over each parameter value and execute the routine
echo "$PARAMETER_VALUES" | while IFS=, read -r PARAMETER_VALUE
do
  if [ "$PARAMETER_VALUE" != "$PARAMETER_COLUMN" ]; then # Skips the header line
    echo "Executing routine with parameter: $PARAMETER_VALUE"
    bq query --use_legacy_sql=false \
      "CALL \`$PROJECT_ID.$DATASET_ID.$ROUTINE_NAME\`('$PARAMETER_VALUE');"
  fi
done


_____<<<<_______<_<<<<<<


#!/bin/bash

# Set BigQuery project and table information
PROJECT_ID="your_project_id"
DATASET_ID="your_dataset_id"
TABLE_ID="email_outbox"

# Temporary file to store email data
EMAIL_DATA_FILE="/tmp/email_data.csv"

# Function to send email using Python
send_email() {
  local email_to="$1"
  local email_subject="$2"
  local email_text="$3"

  python3 <<EOF
import smtplib
from email.mime.text import MIMEText

def send_email(email_to, email_subject, email_text):
    # SMTP server configuration (update these with your server details)
    smtp_server = 'smtp.yourserver.com'
    smtp_port = 587
    smtp_user = 'your_email@domain.com'
    smtp_password = 'your_password'

    msg = MIMEText(email_text)
    msg['Subject'] = email_subject
    msg['From'] = smtp_user
    msg['To'] = email_to

    try:
        with smtplib.SMTP(smtp_server, smtp_port) as server:
            server.starttls()
            server.login(smtp_user, smtp_password)
            server.sendmail(smtp_user, [email_to], msg.as_string())
        print("Email sent successfully")
    except Exception as e:
        print(f"Failed to send email: {e}")

send_email("$email_to", "$email_subject", "$email_text")
EOF
}

# Fetch unsent emails from BigQuery
bq query --use_legacy_sql=false --format=csv "SELECT email_to, email_subject, email_text FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\` WHERE is_sent = 0" > "$EMAIL_DATA_FILE"

# Read the fetched data and process each row
while IFS=, read -r email_to email_subject email_text
do
  if [ "$email_to" != "email_to" ]; then  # Skip the header row
    echo "Sending email to: $email_to"
    send_email "$email_to" "$email_subject" "$email_text"

    # Update the sent flag in BigQuery
    bq query --use_legacy_sql=false "UPDATE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\` SET is_sent = 1 WHERE email_to = '$email_to' AND email_subject = '$email_subject'"
  fi
done < "$EMAIL_DATA_FILE"

# Clean up
rm "$EMAIL_DATA_FILE"

echo "All emails processed."



#!/bin/bash

# Input file with concatenated columns
CONCATENATED_FILE="concatenated_file.csv"
TEMP_CSV_FILE="/tmp/temp_email_data.csv"

# Function to send email using mail command or SMTP client
send_email() {
  local email_to="$1"
  local email_subject="$2"
  local email_text="$3"

  # Use the mail command (adjust to your environment's email-sending method)
  printf "%b" "$email_text" | mail -s "$email_subject" "$email_to"
}

# Convert concatenated file to proper CSV format
# Replace tildes with commas, handle multi-line text
awk -F'~' '
BEGIN { OFS="," }
NR > 1 {
    email_to = $1;
    email_subject = $2;
    email_text = $3;
    
    # Handle potential additional tildes within email_text
    for (i=4; i<=NF; i++) {
        email_text = email_text "~" $i;
    }
    
    # Replace tilde with newline within email_text if needed (example replacement)
    # email_text = gensub(/~/, "\n", "g", email_text);

    # Print in CSV format
    print email_to, email_subject, email_text;
}
' "$CONCATENATED_FILE" > "$TEMP_CSV_FILE"

# Process the CSV file
while IFS=, read -r email_to email_subject email_text
do
  # Remove surrounding quotes from email_text if present
  email_text=$(echo "$email_text" | sed 's/^"//;s/"$//')

  # Convert CSV newlines to actual newlines
  email_text=$(echo "$email_text" | sed 's/\\n/\n/g')

  if [ "$email_to" != "email_to" ]; then  # Skip the header row
    echo "Sending email to: $email_to"
    send_email "$email_to" "$email_subject" "$email_text"

    # Update the sent flag in BigQuery
    bq query --use_legacy_sql=false "UPDATE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\` SET is_sent = 1 WHERE email_to = '$email_to' AND email_subject = '$email_subject'"
  fi
done < "$TEMP_CSV_FILE"

# Clean up
rm "$TEMP_CSV_FILE"

echo "All emails processed."


SELECT 
  email_to,
  email_subject,
  CONCAT('\"', REPLACE(email_text, '\"', '\"\"'), '\"') AS email_text
FROM 
  `${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}`
WHERE 
  is_sent = 0


#!/bin/bash

# Set BigQuery project and table information
PROJECT_ID="your_project_id"
DATASET_ID="your_dataset_id"
TABLE_ID="email_outbox"

# Temporary file to store email data
EMAIL_DATA_FILE="/tmp/email_data.csv"

# Function to send email using mail command or SMTP client
send_email() {
  local email_to="$1"
  local email_subject="$2"
  local email_text="$3"

  # Use the mail command (adjust to your environment's email-sending method)
  printf "%b" "$email_text" | mail -s "$email_subject" "$email_to"
}

# Fetch unsent emails from BigQuery
bq query --use_legacy_sql=false --format=csv "SELECT email_to, email_subject, CONCAT('\"', REPLACE(email_text, '\"', '\"\"'), '\"') AS email_text FROM \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\` WHERE is_sent = 0" > "$EMAIL_DATA_FILE"

# Process the CSV file
awk -F, -v q='"' '
BEGIN { OFS="," }
{
  # Skip header line
  if (NR == 1) {
    next
  }

  # Extract and clean fields
  email_to = $1
  email_subject = $2
  email_text = $3

  # Remove surrounding quotes from email_text if present
  gsub(/^"|"$/, "", email_text)

  # Replace escaped newlines with actual newlines
  gsub(/\\n/, "\n", email_text)

  # Print fields in CSV format
  print email_to, email_subject, email_text
}
' "$EMAIL_DATA_FILE" | while IFS=, read -r email_to email_subject email_text
do
  # Handle multi-line email_text and remove extra quotes
  email_text=$(echo "$email_text" | sed 's/^"//;s/"$//')

  echo "Sending email to: $email_to"
  send_email "$email_to" "$email_subject" "$email_text"

  # Update the sent flag in BigQuery
  bq query --use_legacy_sql=false "UPDATE \`${PROJECT_ID}.${DATASET_ID}.${TABLE_ID}\` SET is_sent = 1 WHERE email_to = '$email_to' AND email_subject = '$email_subject'"
done

# Clean up
rm "$EMAIL_DATA_FILE"

echo "All emails processed."


SELECT e1.employee_name, e1.salary, e1.department_id
FROM employees e1
WHERE e1.salary > (SELECT AVG(e2.salary)
                   FROM employees e2
                   WHERE e2.department_id = e1.department_id);


SELECT e1.employee_name, e1.salary, e1.department_id
FROM employees e1
JOIN (SELECT department_id, AVG(salary) AS avg_salary
      FROM employees
      GROUP BY department_id) avg_salaries
ON e1.department_id = avg_salaries.department_id
WHERE e1.salary > avg_salaries.avg_salary;

CREATE TABLE employees (
    employee_id INT PRIMARY KEY,
    employee_name VARCHAR(100),
    department_id INT,
    salary DECIMAL(10, 2),
    hire_date DATE,
    manager_id INT
);

INSERT INTO employees (employee_id, employee_name, department_id, salary, hire_date, manager_id)
VALUES 
(1, 'John Doe', 101, 60000.00, '2020-03-01', NULL),
(2, 'Jane Smith', 102, 75000.00, '2019-07-15', 1),
(3, 'Alice Johnson', 101, 50000.00, '2021-01-10', 1),
(4, 'Bob Brown', 103, 45000.00, '2018-12-05', 2),
(5, 'Charlie Wilson', 102, 80000.00, '2017-06-20', 2),
(6, 'Emily Davis', 103, 52000.00, '2022-05-16', 4);

-------------------------------------------------------------------------------------------------------------------
#!/bin/bash

DB_SERVER="localhost"
DB_NAME="your_database"
SQL_FILE="/path/to/deploy.sql"

/opt/mssql-tools/bin/sqlcmd -S $DB_SERVER -d $DB_NAME -U sa -P 'YourPassword' -i $SQL_FILE

if [ $? -eq 0 ]; then
    echo "SQL deployment successful!"
else
    echo "SQL deployment failed!"
    exit 1
fi


import requests
import json
from google.cloud import bigquery

# ----- CONFIG -----
BITBUCKET_URL = "https://bitbucket.org/<workspace>/<repo>/raw/<branch>/<path-to-json-file>"
LOCAL_JSON_FILE = "downloaded.json"
PROJECT_ID = "your-project-id"
DATASET_ID = "your_dataset"
TABLE_ID = "your_table"
# ------------------
def convert_multiline_to_ndjson(input_file, output_file):
    with open(input_file, "r", encoding="utf-8") as f:
        data = json.load(f)  # parses as a Python list of dicts

    with open(output_file, "w", encoding="utf-8") as f:
        for record in data:
            f.write(json.dumps(record) + "\n")

    print(f"✅ Converted to NDJSON format: {output_file}")

# 1. Download JSON file from Bitbucket (raw link)
def download_json_file(url, local_path):
    response = requests.get(url)
    if response.status_code == 200:
        with open(local_path, "w", encoding="utf-8") as f:
            f.write(response.text)
        print(f"✅ JSON file downloaded to {local_path}")
    else:
        raise Exception(f"Failed to download file: {response.status_code} - {response.text}")

# 2. Load JSON into BigQuery
def load_json_to_bigquery(local_path, project_id, dataset_id, table_id):
    client = bigquery.Client(project=project_id)
    table_ref = f"{project_id}.{dataset_id}.{table_id}"

    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,
        autodetect=True,  # Or define a schema manually
        write_disposition=bigquery.WriteDisposition.WRITE_APPEND
    )

    with open(local_path, "rb") as source_file:
        job = client.load_table_from_file(source_file, table_ref, job_config=job_config)

    job.result()  # Waits for the job to complete
    print(f"✅ Data loaded into {table_ref}")

# ---- Run All ----
download_json_file(BITBUCKET_URL, LOCAL_JSON_FILE)
load_json_to_bigquery(LOCAL_JSON_FILE, PROJECT_ID, DATASET_ID, TABLE_ID)
CREATE OR REPLACE PROCEDURE `your_project.your_dataset.get_object_dependencies`(
  project_id STRING,
  dataset_id STRING,
  root_object_name STRING
)
BEGIN
  DECLARE object_to_check STRING;
  DECLARE done BOOL DEFAULT FALSE;

  -- Construct full object name
  SET object_to_check = CONCAT(project_id, '.', dataset_id, '.', root_object_name);

  -- Main output table
  CREATE OR REPLACE TABLE `your_project.your_dataset.object_dependencies` (
    level INT64,
    parent_object STRING,
    child_object STRING,
    child_type STRING,
    dependency_path STRING
  );

  -- Optional: Table to track circular references
  CREATE OR REPLACE TABLE `your_project.your_dataset.circular_dependencies` (
    offending_path STRING,
    offending_object STRING
  );

  -- Temp tables
  CREATE TEMP TABLE dependency_hierarchy (
    level INT64,
    parent_object STRING,
    child_object STRING,
    child_type STRING,
    dependency_path STRING
  );

  CREATE TEMP TABLE processing_queue (
    parent_object STRING,
    child_object STRING,
    level INT64,
    dependency_path STRING
  );

  -- Seed with root
  INSERT INTO processing_queue
  VALUES (NULL, object_to_check, 0, object_to_check);

  -- Begin loop
  LOOP
    IF (SELECT COUNT(*) FROM processing_queue) = 0 THEN
      SET done = TRUE;
      LEAVE;
    END IF;

    CREATE TEMP TABLE next_queue AS
    SELECT * FROM processing_queue;

    DELETE FROM processing_queue;

    FOR row IN (SELECT * FROM next_queue) DO
      DECLARE obj_type STRING DEFAULT NULL;
      DECLARE view_sql STRING DEFAULT NULL;

      -- Get object type
      SET obj_type = (
        SELECT table_type
        FROM `your_project`.`your_dataset`.INFORMATION_SCHEMA.TABLES
        WHERE CONCAT(table_catalog, '.', table_schema, '.', table_name) = row.child_object
        LIMIT 1
      );

      -- Insert into hierarchy
      INSERT INTO dependency_hierarchy
      SELECT
        row.level,
        row.parent_object,
        row.child_object,
        obj_type,
        row.dependency_path
      WHERE row.child_object NOT IN (
        SELECT child_object FROM dependency_hierarchy
      );

      -- Handle views
      IF obj_type = 'VIEW' THEN
        SET view_sql = (
          SELECT view_definition
          FROM `your_project`.`your_dataset`.INFORMATION_SCHEMA.VIEWS
          WHERE CONCAT(table_catalog, '.', table_schema, '.', table_name) = row.child_object
          LIMIT 1
        );

        -- Process each referenced object
        FOR ref IN (
          SELECT ref_obj
          FROM UNNEST(REGEXP_EXTRACT_ALL(view_sql, r'`([\w\-]+\.[\w\-]+\.[\w\-]+)`')) AS ref_obj
        ) DO

          -- Check if it's a circular reference in path
          IF REGEXP_CONTAINS(row.dependency_path, r'(' || REGEXP_REPLACE(ref.ref_obj, r'([\.\-])', r'\\\1') || r')') THEN
            INSERT INTO `your_project.your_dataset.circular_dependencies`
            VALUES (row.dependency_path, ref.ref_obj);
          ELSE
            -- Queue it for next level
            INSERT INTO processing_queue
            SELECT
              row.child_object,
              ref.ref_obj,
              row.level + 1,
              row.dependency_path || ' → ' || ref.ref_obj
            WHERE ref.ref_obj NOT IN (
              SELECT child_object FROM dependency_hierarchy
            );
          END IF;

        END FOR;

      END IF;

    END FOR;

    DROP TABLE next_queue;
  END LOOP;

  -- Finalize output
  INSERT INTO `your_project.your_dataset.object_dependencies`
  SELECT * FROM dependency_hierarchy;

END;
-----------------------------------
WITH view_defs AS (
  SELECT
    table_name,
    LOWER(view_definition) AS view_sql
  FROM
    `project_id.dataset_id.INFORMATION_SCHEMA.VIEWS`
),
matches AS (
  SELECT
    table_name,
    REGEXP_EXTRACT_ALL(
      view_sql,
      r'(?i)(?:from|join|left\s+(?:outer\s+)?join|right\s+(?:outer\s+)?join|full\s+(?:outer\s+)?join|inner\s+join|,\s*)\s+`?([a-zA-Z0-9_\-]+(?:\.[a-zA-Z0-9_\-]+){1,2})`?'
    ) AS matched_objects
  FROM view_defs
)
SELECT
  table_name,
  object
FROM matches,
UNNEST(matched_objects) AS object;
--------------------------------------------------------------------------------------
WITH view_defs AS (
  SELECT
    table_name,
    LOWER(REGEXP_REPLACE(view_definition, r'--.*|/\*[\s\S]*?\*/', '')) AS view_sql -- remove comments
  FROM
    `project_id.dataset_id.INFORMATION_SCHEMA.VIEWS`
),
matches AS (
  SELECT
    table_name,
    REGEXP_EXTRACT_ALL(
      view_sql,
      r'(?i)(?:from|join|left\s+(?:outer\s+)?join|right\s+(?:outer\s+)?join|full\s+(?:outer\s+)?join|inner\s+join|cross\s+join|,)\s+`?([a-zA-Z0-9_\-]+(?:\.[a-zA-Z0-9_\-]+){1,2})`?(?=\s|$|as)'
    ) AS matched_objects
  FROM view_defs
)
SELECT
  table_name,
  DISTINCT object
FROM matches,
UNNEST(matched_objects) AS object
WHERE NOT REGEXP_CONTAINS(object, r'^\d+$');  -- eliminate accidental numeric captures
