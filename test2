#!/bin/bash

PROJECT_ID="your-project-id"
BQ_DATASET="gcs_metadata"
BQ_TABLE="bucket_info"
TEMP_FILE="bucket_info.csv"

# Create BigQuery dataset if not exists
bq --location=US mk -d --if_not_exists $BQ_DATASET

# Create table if not exists
bq query --use_legacy_sql=false --format=none \
"CREATE TABLE IF NOT EXISTS \`${PROJECT_ID}.${BQ_DATASET}.${BQ_TABLE}\` (
  bucket_name STRING,
  location STRING,
  storage_class STRING,
  versioning_enabled BOOL,
  time_created TIMESTAMP,
  size_bytes INT64
)"

# Remove old CSV file
rm -f $TEMP_FILE

# Get list of buckets
for BUCKET in $(gsutil ls -p $PROJECT_ID); do
  BUCKET_NAME=$(echo $BUCKET | sed 's/gs:\/\///;s/\/$//')

  # Get bucket metadata
  META=$(gsutil bucketinfo $BUCKET_NAME 2>/dev/null || gsutil ls -L -b gs://$BUCKET_NAME)

  LOCATION=$(echo "$META" | grep -m1 "Location constraint" | awk -F: '{print $2}' | xargs)
  STORAGE_CLASS=$(echo "$META" | grep -m1 "Storage class" | awk -F: '{print $2}' | xargs)
  VERSIONING=$(echo "$META" | grep -m1 "Versioning status" | awk -F: '{print $2}' | xargs)
  TIME_CREATED=$(echo "$META" | grep -m1 "Creation time" | sed 's/.*: //')

  [[ "$VERSIONING" == "Enabled" ]] && VERSIONING_ENABLED=true || VERSIONING_ENABLED=false

  # Get total size in bytes
  SIZE_BYTES=$(gsutil du -s gs://$BUCKET_NAME | awk '{print $1}')

  # Append to temp file
  echo "$BUCKET_NAME,$LOCATION,$STORAGE_CLASS,$VERSIONING_ENABLED,$TIME_CREATED,$SIZE_BYTES" >> $TEMP_FILE
done

# Load CSV into BigQuery
bq load --autodetect --replace --skip_leading_rows=0 \
  --source_format=CSV ${PROJECT_ID}:${BQ_DATASET}.${BQ_TABLE} $TEMP_FILE

echo "Upload complete. Data saved to BigQuery table ${BQ_DATASET}.${BQ_TABLE}."




__________
#!/bin/bash

PROJECT_ID="your-project-id"
BQ_DATASET="gcs_metadata"
BQ_TABLE="bucket_info"
TEMP_FILE="bucket_info.ndjson"

# Create BigQuery dataset if not exists
bq --location=US mk -d --if_not_exists $BQ_DATASET

# Create BigQuery table if not exists
bq query --use_legacy_sql=false --format=none \
"CREATE TABLE IF NOT EXISTS \`${PROJECT_ID}.${BQ_DATASET}.${BQ_TABLE}\` (
  meta_json JSON,
  size_bytes INT64,
  file_count INT64
)"

# Remove previous data file
rm -f $TEMP_FILE

# Loop through each bucket
for BUCKET in $(gsutil ls -p $PROJECT_ID); do
  BUCKET_NAME=$(echo $BUCKET | sed 's/gs:\/\///;s/\/$//')

  echo "Processing bucket: $BUCKET_NAME"

  # Get structured metadata
  META_JSON=$(gcloud storage buckets describe $BUCKET_NAME --project=$PROJECT_ID --format=json)

  # Get total size and object count
  BUCKET_STATS=$(gsutil ls -l -r gs://$BUCKET_NAME/** | grep -v "^TOTAL:" | awk 'BEGIN {total=0; count=0} /^[0-9]/ {total+=$1; count++} END {print total, count}')
  SIZE_BYTES=$(echo $BUCKET_STATS | awk '{print $1}')
  FILE_COUNT=$(echo $BUCKET_STATS | awk '{print $2}')

  # Create NDJSON line (JSON per line format for BigQuery)
  echo "{\"meta_json\": $META_JSON, \"size_bytes\": $SIZE_BYTES, \"file_count\": $FILE_COUNT}" >> $TEMP_FILE
done

# Load NDJSON into BigQuery
bq load --replace --source_format=NEWLINE_DELIMITED_JSON \
  "${PROJECT_ID}:${BQ_DATASET}.${BQ_TABLE}" $TEMP_FILE

echo "âœ… Upload complete. Bucket metadata and size info saved to BigQuery table ${BQ_DATASET}.${BQ_TABLE}."


COUNT=0
MAX_BUCKETS=10

for BUCKET in $(gsutil ls -p $PROJECT_ID); do
  ((COUNT++))
  if [ $COUNT -gt $MAX_BUCKETS ]; then
    break
  fi

  BUCKET_NAME=$(echo $BUCKET | sed 's/gs:\/\///;s/\/$//')

  echo "Processing bucket ($COUNT): $BUCKET_NAME"

  # Get structured metadata
  META_JSON=$(gcloud storage buckets describe $BUCKET_NAME --project=$PROJECT_ID --format=json)

  # Get total size and object count
  BUCKET_STATS=$(gsutil ls -l -r gs://$BUCKET_NAME/** | grep -v "^TOTAL:" | awk 'BEGIN {total=0; count=0} /^[0-9]/ {total+=$1; count++} END {print total, count}')
  SIZE_BYTES=$(echo $BUCKET_STATS | awk '{print $1}')
  FILE_COUNT=$(echo $BUCKET_STATS | awk '{print $2}')

  # Create NDJSON line
  echo "{\"meta_json\": $META_JSON, \"size_bytes\": $SIZE_BYTES, \"file_count\": $FILE_COUNT}" >> $TEMP_FILE
done
----------------------------------------------------------------------------------------------------------------------------------------------------------------
#!/bin/bash

# Usage: ./run_env_ingestion.sh dev|qa|pp

ENV=$1

if [[ "$ENV" != "dev" && "$ENV" != "qa" && "$ENV" != "pp" ]]; then
  echo "Usage: $0 [dev|qa|pp]"
  exit 1
fi

# ENV-SPECIFIC SETTINGS
case $ENV in
  dev)
    SA_KEY="/path/to/dev-sa.json"
    PROJECT_ID="your-dev-project-id"
    BQ_TABLE="bucket_info_dev"
    ;;
  qa)
    SA_KEY="/path/to/qa-sa.json"
    PROJECT_ID="your-qa-project-id"
    BQ_TABLE="bucket_info_qa"
    ;;
  pp)
    SA_KEY="/path/to/pp-sa.json"
    PROJECT_ID="your-pp-project-id"
    BQ_TABLE="bucket_info_pp"
    ;;
esac

# CONSTANT SETTINGS
BQ_DATASET="gcs_metadata"
TMP_FILE="/tmp/${BQ_TABLE}_$(date +%s).ndjson"
MAX_BUCKETS=10

# Activate service account (temporary and scoped to this shell only)
gcloud auth activate-service-account --key-file=$SA_KEY --project=$PROJECT_ID

# Create dataset if not exists
bq --location=US mk -d --if_not_exists "${PROJECT_ID}:${BQ_DATASET}"

# Create table if not exists
bq query --use_legacy_sql=false --format=none \
"CREATE TABLE IF NOT EXISTS \`${PROJECT_ID}.${BQ_DATASET}.${BQ_TABLE}\` (
  meta_json STRING,
  size_bytes INT64,
  file_count INT64
)"

# Clear temp file
rm -f "$TMP_FILE"

# Process up to MAX_BUCKETS
COUNT=0
for BUCKET in $(gsutil ls -p $PROJECT_ID); do
  ((COUNT++))
  if [ $COUNT -gt $MAX_BUCKETS ]; then
    break
  fi

  BUCKET_NAME=$(echo $BUCKET | sed 's/gs:\/\///;s/\/$//')

  echo "[$ENV] Processing bucket: $BUCKET_NAME"

  # Get bucket metadata and convert to single-line JSON string
  META_JSON=$(gcloud storage buckets describe $BUCKET_NAME --project=$PROJECT_ID --format=json | tr -d '\n' | sed 's/"/\\"/g')

  # Get bucket size and object count
  STATS=$(gsutil ls -l -r gs://$BUCKET_NAME/** | grep -v "^TOTAL:" | awk 'BEGIN {s=0;c=0} /^[0-9]/ {s+=$1; c++} END {print s,c}')
  SIZE_BYTES=$(echo $STATS | awk '{print $1}')
  FILE_COUNT=$(echo $STATS | awk '{print $2}')

  # Append to NDJSON
  echo "{\"meta_json\": \"$META_JSON\", \"size_bytes\": $SIZE_BYTES, \"file_count\": $FILE_COUNT}" >> "$TMP_FILE"
done

# Load to BigQuery
bq load --replace --source_format=NEWLINE_DELIMITED_JSON \
  "${PROJECT_ID}:${BQ_DATASET}.${BQ_TABLE}" "$TMP_FILE"

echo "[$ENV]  Ingestion complete. Data loaded to ${BQ_DATASET}.${BQ_TABLE}"

# Optional cleanup
rm -f "$TMP_FILE"


_____________<<<
META_JSON=$(gcloud storage buckets describe $BUCKET_NAME --project=$PROJECT_ID --format=json | tr -d '\n' | sed 's/  */ /g' | sed 's/"/\\"/g')


echo "{\"meta_json\": \"$META_JSON\", \"size_bytes\": $SIZE_BYTES, \"file_count\": $FILE_COUNT}" >> $TEMP_FILE
---------------------------------------------------------------------------------------------------------------------
./run_env_ingestion.sh dev &
./run_env_ingestion.sh qa &
./run_env_ingestion.sh pp &
wait
-----------------------------------------------------------------------------------------
Logging
------------------------------------------------------------------------
import datetime
import smtplib
from email.mime.text import MIMEText
from google.cloud import logging_v2

# Initialize client
client = logging_v2.LoggingServiceV2Client()

# Parameters
PROJECT_ID = "your-project-id"
PRINCIPAL_EMAIL = "principal@example.com"
SENDER_EMAIL = "your_email@gmail.com"
RECEIVER_EMAIL = "receiver@example.com"
EMAIL_PASSWORD = "your_app_password"

# Filter logs for last 5 mins with severity=ERROR and specific principal
def get_log_filter():
    now = datetime.datetime.utcnow()
    start_time = (now - datetime.timedelta(minutes=5)).isoformat("T") + "Z"
    return f'''
        resource.type="gce_instance"
        severity=ERROR
        protoPayload.authenticationInfo.principalEmail="{PRINCIPAL_EMAIL}"
        timestamp >= "{start_time}"
    '''

def format_logs(entries):
    log_details = ""
    for entry in entries:
        timestamp = entry.timestamp
        message = entry.text_payload or str(entry)
        log_name = entry.log_name
        resource = entry.resource.labels
        url = f"https://console.cloud.google.com/logs/query;query={entry.insert_id}?project={PROJECT_ID}"
        log_details += f"""
        Timestamp: {timestamp}
        Log Name: {log_name}
        Resource: {resource}
        Message: {message}
        View in Cloud Logging: {url}
        {'-'*60}
        """
    return log_details

def send_email(subject, body):
    msg = MIMEText(body)
    msg["Subject"] = subject
    msg["From"] = SENDER_EMAIL
    msg["To"] = RECEIVER_EMAIL

    with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
        server.login(SENDER_EMAIL, EMAIL_PASSWORD)
        server.sendmail(SENDER_EMAIL, RECEIVER_EMAIL, msg.as_string())

def main():
    filter_str = get_log_filter()
    parent = f"projects/{PROJECT_ID}"
    
    entries = client.list_log_entries(
        {"resource_names": [parent], "filter": filter_str}
    )

    entries = list(entries)
    if entries:
        body = format_logs(entries)
        send_email("GCP Logging Alert: Error Detected", body)
    else:
        print("No matching logs found.")

if __name__ == "__main__":
    main()
-------------------------------------------------------------------------------------
With BigQuery
------------------------------------------------------------------------------------
import datetime
import smtplib
from email.mime.text import MIMEText
from google.cloud import bigquery

# === CONFIG ===
BQ_PROJECT = "your-project-id"
BQ_DATASET = "your_dataset"
BQ_TABLE = "audit_logs"
PROJECT_ID = "your-project-id"
PRINCIPAL_EMAIL = "user@example.com"  # Optional, or set to None
EMAIL_FROM = "your_email@gmail.com"
EMAIL_TO = "team@example.com"
EMAIL_PASSWORD = "your_app_password"

# === QUERY AUDIT LOGS IN BQ ===
def query_audit_errors():
    client = bigquery.Client(project=BQ_PROJECT)
    now = datetime.datetime.utcnow()
    start_time = (now - datetime.timedelta(minutes=5)).isoformat()

    email_filter = f"""AND JSON_VALUE(protoPayload.authenticationInfo.principalEmail) = '{PRINCIPAL_EMAIL}'""" if PRINCIPAL_EMAIL else ""

    query = f"""
    SELECT
      timestamp,
      JSON_VALUE(protoPayload.authenticationInfo.principalEmail) AS principalEmail,
      JSON_VALUE(protoPayload.methodName) AS methodName,
      JSON_VALUE(protoPayload.resourceName) AS resourceName,
      JSON_VALUE(protoPayload.status.message) AS errorMessage,
      insertId
    FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}`
    WHERE severity = 'ERROR'
      AND timestamp >= TIMESTAMP('{start_time}')
      {email_filter}
    ORDER BY timestamp DESC
    """

    return client.query(query).result()

# === FORMAT LOGS ===
def format_results(results):
    body = ""
    for row in results:
        log_filter = f'resource.type="project" AND insertId="{row.insertId}"'
        log_url = f"https://console.cloud.google.com/logs/query;query={log_filter.replace(' ', '%20').replace('\"', '%22')}?project={PROJECT_ID}"

        body += f"""
Timestamp     : {row.timestamp}
Principal     : {row.principalEmail}
Method        : {row.methodName}
Resource      : {row.resourceName}
Error Message : {row.errorMessage}
View in GCP   : {log_url}
{'-'*60}
"""
    return body

# === SEND EMAIL ===
def send_email(subject, body):
    msg = MIMEText(body)
    msg["Subject"] = subject
    msg["From"] = EMAIL_FROM
    msg["To"] = EMAIL_TO

    with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
        server.login(EMAIL_FROM, EMAIL_PASSWORD)
        server.sendmail(EMAIL_FROM, EMAIL_TO, msg.as_string())

# === MAIN ===
def main():
    results = query_audit_errors()
    rows = list(results)

    if rows:
        body = format_results(rows)
        send_email("BigQuery Audit Logs Alert: Errors Detected", body)
        print("Alert sent.")
    else:
        print("No error entries found.")

if __name__ == "__main__":
    main()
